{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb4b6c3",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922599c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "\n",
    "# Set theme\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34862012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load routing results\n",
    "# Replace with your actual results file path\n",
    "results_df = pd.read_csv(\"../data/routing_results.csv\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Total requests: {len(results_df):,}\")\n",
    "print(f\"Columns: {list(results_df.columns)}\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d853fd",
   "metadata": {},
   "source": [
    "## 2. Routing Decision Distribution\n",
    "\n",
    "Visualize how requests were routed: weak only, strong only, or weak â†’ strong (evaluator triggered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count routing decisions\n",
    "routing_counts = results_df[\"model_used\"].value_counts()\n",
    "\n",
    "# Create labels with percentages\n",
    "labels = []\n",
    "values = []\n",
    "for model, count in routing_counts.items():\n",
    "    pct = (count / len(results_df)) * 100\n",
    "    labels.append(f\"{model}<br>({pct:.1f}%)\")\n",
    "    values.append(count)\n",
    "\n",
    "# Create pie chart\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=labels,\n",
    "    values=values,\n",
    "    hole=0.4,\n",
    "    marker_colors=['#2ecc71', '#e74c3c', '#f39c12'],\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Routing Decision Distribution\",\n",
    "    annotations=[dict(text='Routing<br>Decisions', x=0.5, y=0.5, font_size=14, showarrow=False)],\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50fb85",
   "metadata": {},
   "source": [
    "## 3. Cost Comparison Analysis\n",
    "\n",
    "Compare costs between our smart routing system and baseline (always using strong model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total and average costs\n",
    "total_cost = results_df[\"cost_usd\"].sum()\n",
    "avg_cost = results_df[\"cost_usd\"].mean()\n",
    "\n",
    "# Calculate baseline (always strong) cost\n",
    "# Assuming strong model costs 20x more than weak\n",
    "baseline_cost = total_cost * 2.5  # Placeholder - adjust based on your data\n",
    "\n",
    "# Prepare data\n",
    "cost_data = pd.DataFrame({\n",
    "    \"Strategy\": [\"Smart Router\", \"Always Strong (Baseline)\"],\n",
    "    \"Total Cost ($)\": [total_cost, baseline_cost],\n",
    "    \"Avg Cost per Request ($)\": [avg_cost, baseline_cost / len(results_df)],\n",
    "})\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Total Cost',\n",
    "    x=cost_data[\"Strategy\"],\n",
    "    y=cost_data[\"Total Cost ($)\"],\n",
    "    text=[f\"${v:.2f}\" for v in cost_data[\"Total Cost ($)\"]],\n",
    "    textposition='outside',\n",
    "    marker_color=['#2ecc71', '#e74c3c'],\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Cost Comparison<br><sub>Savings: ${baseline_cost - total_cost:.2f} ({(1 - total_cost/baseline_cost)*100:.1f}%)</sub>\",\n",
    "    yaxis_title=\"Total Cost (USD)\",\n",
    "    showlegend=False,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost breakdown by routing decision\n",
    "cost_by_model = results_df.groupby(\"model_used\")[\"cost_usd\"].agg(['sum', 'mean', 'count'])\n",
    "cost_by_model['total_contribution'] = cost_by_model['sum'] / total_cost * 100\n",
    "\n",
    "print(\"Cost breakdown by routing decision:\")\n",
    "print(cost_by_model)\n",
    "\n",
    "# Visualize cost contribution\n",
    "fig = px.bar(\n",
    "    cost_by_model.reset_index(),\n",
    "    x='model_used',\n",
    "    y='sum',\n",
    "    color='model_used',\n",
    "    text=[f\"${v:.2f}<br>({p:.1f}%)\" for v, p in zip(cost_by_model['sum'], cost_by_model['total_contribution'])],\n",
    "    labels={'model_used': 'Routing Decision', 'sum': 'Total Cost ($)'},\n",
    "    title=\"Cost Contribution by Routing Decision\",\n",
    "    color_discrete_map={'weak': '#2ecc71', 'strong': '#e74c3c', 'weak_then_strong': '#f39c12'},\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.update_layout(showlegend=False, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5194b4b9",
   "metadata": {},
   "source": [
    "## 4. Quality Score Distributions\n",
    "\n",
    "Analyze the distribution of quality scores from the evaluator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4455c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for cases where weak model was evaluated\n",
    "weak_evaluated = results_df[results_df[\"weak_quality_score\"].notna()]\n",
    "\n",
    "print(f\"Weak responses evaluated: {len(weak_evaluated):,}\")\n",
    "print(f\"Mean quality score: {weak_evaluated['weak_quality_score'].mean():.3f}\")\n",
    "print(f\"Median quality score: {weak_evaluated['weak_quality_score'].median():.3f}\")\n",
    "\n",
    "# Create histogram with threshold line\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=weak_evaluated[\"weak_quality_score\"],\n",
    "    nbinsx=50,\n",
    "    name=\"Quality Scores\",\n",
    "    marker_color='#3498db',\n",
    "    opacity=0.7,\n",
    "))\n",
    "\n",
    "# Add threshold line\n",
    "evaluator_threshold = 0.7  # Adjust to your config\n",
    "fig.add_vline(\n",
    "    x=evaluator_threshold,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=f\"Threshold ({evaluator_threshold})\",\n",
    "    annotation_position=\"top right\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Weak Response Quality Scores\",\n",
    "    xaxis_title=\"Quality Score\",\n",
    "    yaxis_title=\"Count\",\n",
    "    showlegend=False,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022adbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot comparing quality scores by final routing decision\n",
    "fig = px.box(\n",
    "    weak_evaluated,\n",
    "    x=\"model_used\",\n",
    "    y=\"weak_quality_score\",\n",
    "    color=\"model_used\",\n",
    "    title=\"Quality Score Distribution by Routing Decision\",\n",
    "    labels={\"model_used\": \"Final Routing\", \"weak_quality_score\": \"Quality Score\"},\n",
    "    color_discrete_map={'weak': '#2ecc71', 'weak_then_strong': '#f39c12'},\n",
    ")\n",
    "\n",
    "fig.add_hline(\n",
    "    y=evaluator_threshold,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=\"Threshold\",\n",
    ")\n",
    "\n",
    "fig.update_layout(showlegend=False, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de362f",
   "metadata": {},
   "source": [
    "## 5. Latency Analysis\n",
    "\n",
    "Examine request latency across different routing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2597bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency statistics\n",
    "latency_stats = results_df.groupby(\"model_used\")[\"latency_ms\"].agg([\n",
    "    ('Mean', 'mean'),\n",
    "    ('Median', 'median'),\n",
    "    ('P95', lambda x: x.quantile(0.95)),\n",
    "    ('P99', lambda x: x.quantile(0.99)),\n",
    "])\n",
    "\n",
    "print(\"Latency statistics by routing decision (ms):\")\n",
    "print(latency_stats.round(0))\n",
    "\n",
    "# Violin plot\n",
    "fig = px.violin(\n",
    "    results_df,\n",
    "    x=\"model_used\",\n",
    "    y=\"latency_ms\",\n",
    "    color=\"model_used\",\n",
    "    box=True,\n",
    "    points=\"outliers\",\n",
    "    title=\"Latency Distribution by Routing Decision\",\n",
    "    labels={\"model_used\": \"Routing Decision\", \"latency_ms\": \"Latency (ms)\"},\n",
    "    color_discrete_map={'weak': '#2ecc71', 'strong': '#e74c3c', 'weak_then_strong': '#f39c12'},\n",
    ")\n",
    "\n",
    "fig.update_layout(showlegend=False, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985ef74",
   "metadata": {},
   "source": [
    "## 6. Cost-Quality Trade-off Curves\n",
    "\n",
    "Explore the relationship between cost and quality at different threshold settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate different threshold settings\n",
    "# This would ideally use actual data from multiple threshold experiments\n",
    "\n",
    "thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "simulated_data = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Simulate: higher threshold = more weak usage = lower cost but potentially lower quality\n",
    "    strong_pct = (1 - threshold) * 100\n",
    "    cost_factor = 0.2 + (strong_pct / 100) * 0.8  # Cost relative to always-strong\n",
    "    quality_factor = 0.7 + (strong_pct / 100) * 0.3  # Quality relative to always-strong\n",
    "    \n",
    "    simulated_data.append({\n",
    "        \"threshold\": threshold,\n",
    "        \"strong_model_usage_pct\": strong_pct,\n",
    "        \"relative_cost\": cost_factor,\n",
    "        \"relative_quality\": quality_factor,\n",
    "    })\n",
    "\n",
    "sim_df = pd.DataFrame(simulated_data)\n",
    "\n",
    "# Create trade-off curve\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sim_df[\"relative_cost\"] * 100,\n",
    "    y=sim_df[\"relative_quality\"] * 100,\n",
    "    mode='lines+markers',\n",
    "    name='Cost-Quality Frontier',\n",
    "    text=[f\"Threshold: {t:.1f}<br>Strong: {s:.0f}%\" for t, s in zip(sim_df[\"threshold\"], sim_df[\"strong_model_usage_pct\"])],\n",
    "    hovertemplate='<b>Cost:</b> %{x:.1f}%<br><b>Quality:</b> %{y:.1f}%<br>%{text}<extra></extra>',\n",
    "    marker=dict(size=10, color=sim_df[\"threshold\"], colorscale='Viridis', showscale=True, colorbar=dict(title=\"Threshold\")),\n",
    "    line=dict(width=2),\n",
    "))\n",
    "\n",
    "# Add current operating point\n",
    "current_threshold = 0.7\n",
    "current_point = sim_df[sim_df[\"threshold\"] == current_threshold].iloc[0]\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[current_point[\"relative_cost\"] * 100],\n",
    "    y=[current_point[\"relative_quality\"] * 100],\n",
    "    mode='markers',\n",
    "    name='Current Setting',\n",
    "    marker=dict(size=15, color='red', symbol='star'),\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Cost vs Quality Trade-off Curve\",\n",
    "    xaxis_title=\"Relative Cost (%)\",\n",
    "    yaxis_title=\"Relative Quality (%)\",\n",
    "    height=600,\n",
    "    hovermode='closest',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c738a5e",
   "metadata": {},
   "source": [
    "## 7. Routing Score vs Actual Decision\n",
    "\n",
    "Analyze how well the RouteLLM routing score correlates with final routing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: routing score vs cost\n",
    "fig = px.scatter(\n",
    "    results_df,\n",
    "    x=\"routing_score\",\n",
    "    y=\"cost_usd\",\n",
    "    color=\"model_used\",\n",
    "    title=\"Routing Score vs Actual Cost\",\n",
    "    labels={\"routing_score\": \"RouteLLM Score (Strong Win Rate)\", \"cost_usd\": \"Cost ($)\"},\n",
    "    color_discrete_map={'weak': '#2ecc71', 'strong': '#e74c3c', 'weak_then_strong': '#f39c12'},\n",
    "    opacity=0.6,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "# Add routing threshold line\n",
    "routing_threshold = 0.5  # Adjust to your config\n",
    "fig.add_vline(\n",
    "    x=routing_threshold,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"purple\",\n",
    "    annotation_text=f\"Router Threshold ({routing_threshold})\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450fe019",
   "metadata": {},
   "source": [
    "## 8. Summary Dashboard\n",
    "\n",
    "Create a comprehensive dashboard with key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb50ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics\n",
    "total_requests = len(results_df)\n",
    "weak_only = (results_df[\"model_used\"] == \"weak\").sum()\n",
    "strong_only = (results_df[\"model_used\"] == \"strong\").sum()\n",
    "weak_then_strong = (results_df[\"model_used\"] == \"weak_then_strong\").sum()\n",
    "avg_cost = results_df[\"cost_usd\"].mean()\n",
    "total_cost = results_df[\"cost_usd\"].sum()\n",
    "avg_latency = results_df[\"latency_ms\"].mean()\n",
    "evaluator_trigger_rate = (weak_then_strong / total_requests) * 100\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=3,\n",
    "    subplot_titles=(\n",
    "        \"Total Requests\", \"Avg Cost/Request\", \"Avg Latency\",\n",
    "        \"Strong Model Usage\", \"Evaluator Trigger Rate\", \"Total Cost\"\n",
    "    ),\n",
    "    specs=[\n",
    "        [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}, {\"type\": \"indicator\"}],\n",
    "        [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}, {\"type\": \"indicator\"}]\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Add indicators\n",
    "fig.add_trace(go.Indicator(\n",
    "    mode=\"number\",\n",
    "    value=total_requests,\n",
    "    number={\"font\": {\"size\": 50}},\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Indicator(\n",
    "    mode=\"number+delta\",\n",
    "    value=avg_cost,\n",
    "    number={\"prefix\": \"$\", \"font\": {\"size\": 50}},\n",
    "    delta={\"reference\": avg_cost * 2, \"relative\": True},\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Indicator(\n",
    "    mode=\"number\",\n",
    "    value=avg_latency,\n",
    "    number={\"suffix\": \"ms\", \"font\": {\"size\": 50}},\n",
    "), row=1, col=3)\n",
    "\n",
    "fig.add_trace(go.Indicator(\n",
    "    mode=\"gauge+number\",\n",
    "    value=(strong_only + weak_then_strong) / total_requests * 100,\n",
    "    number={\"suffix\": \"%\", \"font\": {\"size\": 40}},\n",
    "    gauge={\"axis\": {\"range\": [0, 100]}, \"bar\": {\"color\": \"#e74c3c\"}},\n",
    "), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Indicator(\n",
    "    mode=\"gauge+number\",\n",
    "    value=evaluator_trigger_rate,\n",
    "    number={\"suffix\": \"%\", \"font\": {\"size\": 40}},\n",
    "    gauge={\"axis\": {\"range\": [0, 100]}, \"bar\": {\"color\": \"#f39c12\"}},\n",
    "), row=2, col=2)\n",
    "\n",
    "fig.add_trace(go.Indicator(\n",
    "    mode=\"number\",\n",
    "    value=total_cost,\n",
    "    number={\"prefix\": \"$\", \"font\": {\"size\": 50}},\n",
    "), row=2, col=3)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Smart LLM Routing - Summary Dashboard\",\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090dc6c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides comprehensive visualization of the smart LLM routing system's performance:\n",
    "\n",
    "- **Cost Savings**: Significant reduction compared to always-strong baseline\n",
    "- **Quality Maintenance**: Evaluator ensures quality by triggering strong model when needed\n",
    "- **Latency Trade-offs**: Understand performance characteristics of different routing decisions\n",
    "- **Optimization Opportunities**: Use threshold tuning to find optimal cost-quality balance\n",
    "\n",
    "Next steps:\n",
    "1. Run more experiments with different threshold settings\n",
    "2. Collect human evaluation data for quality assessment\n",
    "3. A/B test against production traffic\n",
    "4. Monitor metrics in real-time with dashboard"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
